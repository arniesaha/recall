# Recall ğŸ§ 

Your personal knowledge system. Search meeting transcripts, notes, decisions, and PDFs with AI-powered retrieval.

> *"What did we decide about X?" â€” answered in seconds.*

## Features

- **Hybrid Search** â€” BM25 keyword + vector semantic search with RRF fusion
- **PDF Support** â€” Index PDFs with page-aware chunking
- **RAG Answers** â€” Natural language questions answered with context
- **1:1 Prep** â€” Quick context for meetings with specific people
- **Query Expansion** â€” LLM generates alternative phrasings
- **LLM Reranking** â€” Position-aware reranking for quality

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Recall API                                â”‚
â”‚                    (FastAPI + Python)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LanceDB     â”‚    â”‚ SQLite FTS5   â”‚    â”‚    Ollama     â”‚
â”‚   (vectors)   â”‚    â”‚   (BM25)      â”‚    â”‚  (embeddings) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Search Modes

| Mode | Speed | Quality | Description |
|------|-------|---------|-------------|
| `hybrid` | Fast | Better | BM25 + Vector + RRF fusion (recommended) |
| `query` | Slow | Best | Expansion + hybrid + reranking |
| `vector` | Fast | Good | Pure semantic search |
| `bm25` | Fast | Good | Pure keyword search |

## Quick Start

### Prerequisites

- Docker & Docker Compose
- Ollama with embedding model (`nomic-embed-text`)
- Markdown files to index (Obsidian vault, Granola transcripts, etc.)

### Setup

1. Clone and configure:

```bash
git clone https://github.com/arniesaha/note-rag.git
cd note-rag/services
cp .env.example .env
# Edit .env with your settings
```

2. Start services:

```bash
docker compose up -d
```

3. Pull embedding model:

```bash
docker exec -it kg-ollama ollama pull nomic-embed-text
docker exec -it kg-ollama ollama pull qwen2.5:0.5b  # For reranking
```

4. Index your documents:

```bash
curl -X POST http://localhost:8080/index/start \
  -H "Authorization: Bearer your-api-token" \
  -H "Content-Type: application/json" \
  -d '{"vault": "all", "full": true}'
```

## API Endpoints

### Search

```bash
curl -X POST http://localhost:8080/search \
  -H "Authorization: Bearer your-api-token" \
  -H "Content-Type: application/json" \
  -d '{"query": "project timeline", "mode": "hybrid", "limit": 10}'
```

### RAG Query

```bash
curl -X POST http://localhost:8080/query \
  -H "Authorization: Bearer your-api-token" \
  -H "Content-Type: application/json" \
  -d '{"question": "What did we decide about the API redesign?"}'
```

### 1:1 Prep

```bash
curl http://localhost:8080/prep/PersonName \
  -H "Authorization: Bearer your-api-token"
```

### Health Check

```bash
curl http://localhost:8080/health
```

## Key Algorithms

### Reciprocal Rank Fusion (RRF)

Combines multiple ranked lists:

```
score = Î£ 1/(k + rank + 1)  where k=60
```

Plus top-rank bonus: +0.05 for #1, +0.02 for #2-3.

### Position-Aware Reranking

Blends retrieval scores with LLM reranker:

- Top 1-3: 75% retrieval, 25% reranker (preserve exact matches)
- Top 4-10: 60% retrieval, 40% reranker
- Top 11+: 40% retrieval, 60% reranker (trust reranker more)

## Project Structure

```
note-rag/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py          # FastAPI application
â”‚   â”‚   â”œâ”€â”€ searcher.py      # Search logic (hybrid, RRF)
â”‚   â”‚   â”œâ”€â”€ indexer.py       # Document indexing
â”‚   â”‚   â”œâ”€â”€ fts_index.py     # SQLite FTS5 wrapper
â”‚   â”‚   â”œâ”€â”€ fusion.py        # RRF implementation
â”‚   â”‚   â”œâ”€â”€ reranker.py      # LLM reranking
â”‚   â”‚   â”œâ”€â”€ config.py        # Settings
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ daily_sync.py         # Sync from sources
â”‚   â””â”€â”€ cleanup_sources.py
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ QMD-PORT-ROADMAP.md
â””â”€â”€ n8n/                       # n8n workflow configs
```

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `API_TOKEN` | Authentication token | (required) |
| `OLLAMA_URL` | Ollama API endpoint | `http://ollama:11434` |
| `LANCEDB_PATH` | Vector DB storage path | `/data/lancedb` |
| `VAULT_WORK_PATH` | Work vault path | `/data/obsidian/work` |
| `VAULT_PERSONAL_PATH` | Personal vault path | `/data/obsidian/personal` |
| `EXCLUDED_FOLDERS` | Folders to skip | `personal/finance` |
| `LOG_LEVEL` | Logging level | `INFO` |

### Models Used

| Model | Purpose | Size |
|-------|---------|------|
| `nomic-embed-text` | Embeddings | ~275MB |
| `qwen2.5:0.5b` | Reranking + query expansion | ~400MB |

## Development

```bash
# Install dependencies
cd services/api
pip install -r requirements.txt

# Run locally
uvicorn main:app --reload --port 8080
```

## Credits

- Search pipeline inspired by [QMD](https://github.com/tobi/qmd) by Tobi LÃ¼tke
- Vector search: [LanceDB](https://lancedb.com/)
- Embeddings: [Ollama](https://ollama.ai/) + nomic-embed-text

## License

MIT
